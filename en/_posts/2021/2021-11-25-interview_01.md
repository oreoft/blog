---
category: other
excerpt: Things I ran into during interviews â€” slightly better than the usual rote
  Q&A drills
keywords: other, java
lang: en
layout: post
title: Sharing Three Code Design Lessons I Ran Into Today
---

## Preface

Today I had the second-round interview with my Dream Company. The interviewer moved super fast, and it was a phone interview tooâ€”he basically didnâ€™t let you talk much, just listened for the conclusion he wanted.... That made me a bit nervous, plus I really cared about this interview... so I got even more nervous ğŸ˜­. I donâ€™t feel like I performed particularly wellâ€”maybe thatâ€™s just how big companies are, but honestly itâ€™s still just me being not good enough..... Iâ€™ve also been an interviewer myself and talked with candidates. From my perspective, our goals are aligned: I want to know the candidateâ€™s real level, and the candidate wants to show their real level. But todayâ€™s second round felt like it was with a technical leaderâ€”probably busy, and interviewing a lot of people.

Still, I did get something out of it. I heard some design questions that were different from the usual ones. After the interview I thought about them a bit and found them pretty interesting, so Iâ€™m summarizing and reviewing them here. Of course, there were also lots of questions like **designing a leaderboard**, which everyone asks all the time and which Iâ€™ve also **implemented broadly** in my own projectsâ€”so I wonâ€™t write about those.

The related code for this post has been uploaded to [github](https://github.com/oreoft/blog-examples/tree/main/interview-design1125)

## How do you deduplicate 4 billion QQ numbers?

This is actually a pretty common type of question. For example: `massive log data: extract the IP with the most Baidu visits on a given day, find hot queries, count the top 10 hottest queries among 3 million query strings, given file a and file b each storing 5 billion URLs (64 bytes each), memory limit 4G, find the common URLs between a and b?` These are all classic massive-data design problems. Most â€œstandard answersâ€ use a bitmap to balance time and space. But since Iâ€™ve done big data related work before, if itâ€™s deduplication Iâ€™d lean more toward **using MapReduce**. Of course, if the interviewer asked for median or top-k, then Iâ€™d definitely use a bitmap and do one pass of sorting to finish it.

If the question is purely about deduplication, the first thing most people think of is hashing. But if you directly use a hash set, memory usage will exceed the limit. And MapReduce exists to solve exactly this kind of problem. MapReduce is a computation model: it decomposes work or data for large-scale processing, and finally merges the results. Itâ€™s essentially like merge sort in spiritâ€”divide and conquer. Hadoop is an open-source distributed parallel processing framework that implements the MapReduce model.

So for QQ number deduplication, we just map/split the data and hand it off to different machines to process (this is the map phase), then continuously reduce/aggregate (this is the reduce phase). Concretely, we can:

1. First use a mapping method, e.g. `%10000`, to split the file containing QQ numbers. This produces 10,000 files, then distribute them to different nodes. Each node is responsible for its own two files.
2. After the big file becomes small files, each node can load its file into memory and deduplicate locally. Using a normal hash-based dedup is fine here.
3. Each pair of small files will eventually become an even smaller deduplicated file, and then we keep merging files from two nodes, merging all the way until thereâ€™s only one file left.

With this kind of data partitioning and final reduction, you end up with one deduplicated file. This theory can also answer problems like hash counting and hash dedup. Bitmap is a great answer, but answering with MapReduce wonâ€™t lose points either. In fact, these scenario design questions mainly test your usual accumulation and ideas. At least I wouldnâ€™t restrict candidates from brainstorming (and my interviewer didnâ€™t eitherâ€”he actuallyè®¤å¯â€™d this answer, hhhh).



## How is the WeChat red packet amount split?

### Rough idea

I feel like I saw this question before on some public account, but I forgot after reading it. Today I got asked it out of nowhere. My first reaction wasnâ€™t to answerâ€”I genuinely wanted to know. That damn curiosity. Things we take for granted every day, we rarely think about from an engineerâ€™s perspective. Honestly, thatâ€™s on me.

Once I heard the question, I realized it was a blind spot. I was super nervous. I donâ€™t even remember how many seconds I thought before blurting something out. My idea was: since itâ€™s â€œgrabbingâ€ a red packet, it must be random. So how do we randomize? We need to determine the upper and lower bounds for each personâ€™s random amount. **The lower bound is naturally 1 cent per person**, and **the upper bound is remaining amount - number of people * 1 cent** (because we must guarantee everyone gets at least 1 cent). So the random range is **[1 cent, remaining amount - people * 1 cent]**. But this distribution is unbalanced, i.e., unfair: early grabbers have a big advantage because they can random a larger number, and the later you are, the smaller your expected share becomes. I actually wrote some code to test itâ€”under the default random seed, after a few rounds the big chunk gets taken early.

> Suppose there are 10 people, total red packet amount is 100 cents.
>
> The first personâ€™s random range is **(1, 100 - 9)**, expected value is **45.5 cents**, suppose this person got 45.5
>
> The second personâ€™s random range is **(1, 45.5 - 8)**, expected value is **18.75 cents**, suppose this person got 18.75
>
> The second personâ€™s random range is **(1, 18.75 - 7)**, expected value is **11.75 cents**, suppose this person got 11.75
>
> ......
>
> Although Iâ€™m using expected values in the example, itâ€™s inevitable that it gets smaller and smaller as you go.



### Thinking more carefully

Afterwards, thinking carefully, I realized this approach is actually workable. Determining the random upper/lower bounds is the first step. The problem is the current upper bound is unfair. So if we make the upper bound dynamically adjusted to a reasonable range based on the remaining people, it could be better. But how do we find that value? I searched around and found something called the **double-mean method**. It uses the double-mean method to determine each personâ€™s upper bound: **divide the remaining amount by the remaining number of people, then multiply by 2**. So the range becomes **[1, remaining amount / remaining people * 2]**. My understanding is that this upper bound is adjusted each time based on remaining amount and remaining people, which can help keep the distribution fair to some extent. Since it follows a normal distribution, in terms of overall mathematical expectation itâ€™s actually fair. For example:

> Suppose there are 10 people, total red packet amount is 100 cents.
>
> The first personâ€™s random range is (1, 100/10*2), expected 10 cents, suppose this person got 10
>
> The second personâ€™s random range is (1, 90/9*2), also expected 10 cents, suppose this person got 10
>
> ....
>
> The tenth personâ€™s random range is (1, 10/1*2), expected 10 cents, suppose this person got 10

If it follows this distribution, everyone ends up with roughly the same amount. But since itâ€™s random, youâ€™ll still see fluctuations around the mean, like:

> Suppose there are 10 people, total red packet amount is 100 cents.
>
> The first personâ€™s random range is (1, 100/10*2), suppose this person got 15
>
> The second personâ€™s random range is (1, 85/9*2 = 18.888), suppose this person got 15
>
> The third personâ€™s random range is (1, 70/9*2 = 15.555), suppose this person got 15
>
> ...

You can see that in this case itâ€™s still not very fair for the later people, because the first personâ€™s [0, 20] rangeâ€”just because the first person randomly got a bit moreâ€”shrinks the later ranges. But dynamically adjusting the upper bound can at least prevent the values from diverging too much. I implemented it with code and the results looked okay.

**The code is too long, so I uploaded it to [github](https://github.com/oreoft/blog-examples/tree/main/interview-design1125).**

```java
30ä¸ªäººæŠ¢100åˆ†é’±
ç¬¬1æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹96åˆ†, åŒºé—´æ˜¯[1, 6.666667]
ç¬¬2æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹91åˆ†, åŒºé—´æ˜¯[1, 6.620690]
ç¬¬3æŠ¢åˆ°äº†2åˆ†, è¿˜å‰©ä¸‹89åˆ†, åŒºé—´æ˜¯[1, 6.500000]
ç¬¬4æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹88åˆ†, åŒºé—´æ˜¯[1, 6.592593]
ç¬¬5æŠ¢åˆ°äº†3åˆ†, è¿˜å‰©ä¸‹85åˆ†, åŒºé—´æ˜¯[1, 6.769231]
ç¬¬6æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹81åˆ†, åŒºé—´æ˜¯[1, 6.800000]
ç¬¬7æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹77åˆ†, åŒºé—´æ˜¯[1, 6.750000]
ç¬¬8æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹76åˆ†, åŒºé—´æ˜¯[1, 6.695652]
ç¬¬9æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹71åˆ†, åŒºé—´æ˜¯[1, 6.909091]
ç¬¬10æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹67åˆ†, åŒºé—´æ˜¯[1, 6.761905]
ç¬¬11æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹62åˆ†, åŒºé—´æ˜¯[1, 6.700000]
ç¬¬12æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹57åˆ†, åŒºé—´æ˜¯[1, 6.526316]
ç¬¬13æŠ¢åˆ°äº†6åˆ†, è¿˜å‰©ä¸‹51åˆ†, åŒºé—´æ˜¯[1, 6.333333]
ç¬¬14æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹47åˆ†, åŒºé—´æ˜¯[1, 6.000000]
ç¬¬15æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹46åˆ†, åŒºé—´æ˜¯[1, 5.875000]
ç¬¬16æŠ¢åˆ°äº†3åˆ†, è¿˜å‰©ä¸‹43åˆ†, åŒºé—´æ˜¯[1, 6.133333]
ç¬¬17æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹42åˆ†, åŒºé—´æ˜¯[1, 6.142857]
ç¬¬18æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹38åˆ†, åŒºé—´æ˜¯[1, 6.461538]
ç¬¬19æŠ¢åˆ°äº†2åˆ†, è¿˜å‰©ä¸‹36åˆ†, åŒºé—´æ˜¯[1, 6.333333]
ç¬¬20æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹31åˆ†, åŒºé—´æ˜¯[1, 6.545455]
ç¬¬21æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹30åˆ†, åŒºé—´æ˜¯[1, 6.200000]
ç¬¬22æŠ¢åˆ°äº†6åˆ†, è¿˜å‰©ä¸‹24åˆ†, åŒºé—´æ˜¯[1, 6.666667]
ç¬¬23æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹23åˆ†, åŒºé—´æ˜¯[1, 6.000000]
ç¬¬24æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹19åˆ†, åŒºé—´æ˜¯[1, 6.571429]
ç¬¬25æŠ¢åˆ°äº†3åˆ†, è¿˜å‰©ä¸‹16åˆ†, åŒºé—´æ˜¯[1, 6.333333]
ç¬¬26æŠ¢åˆ°äº†4åˆ†, è¿˜å‰©ä¸‹12åˆ†, åŒºé—´æ˜¯[1, 6.400000]
ç¬¬27æŠ¢åˆ°äº†5åˆ†, è¿˜å‰©ä¸‹7åˆ†, åŒºé—´æ˜¯[1, 6.000000]
ç¬¬28æŠ¢åˆ°äº†1åˆ†, è¿˜å‰©ä¸‹6åˆ†, åŒºé—´æ˜¯[1, 4.666667]
ç¬¬29æŠ¢åˆ°äº†3åˆ†, è¿˜å‰©ä¸‹3åˆ†, åŒºé—´æ˜¯[1, 6.000000]
æœ€åä¸€ä¸ªå·²ç»æŠ¢å®Œ, é‡‘é¢ä¸º3
å·²ç»æŠ¢å®Œ, æ€»å…±é‡‘é¢ä¸º100åˆ†
```

Besides that, I also found a drawback: the last person is absolutely treated unfairly, because they can only take whatever is left after everyone else is done. I thought about it carefully and this is really a paradox: on one hand itâ€™s random, on the other hand you want everyone to have as close as possible to the same probability distribution of amounts. I read a lot of blogs and most people arenâ€™t very satisfied with this either. They propose a â€œ**linear cutting method**â€ algorithm with higher time and complexity. The idea is to abstract the total amount as a rope and cut it, making a total of (people - 1) cuts. Then each personâ€™s amount is the proportion of rope they get. I wanted to implement it, but there are too many things to consider and itâ€™s pretty hard, so I gave up. [Reference 3](https://www.cnblogs.com/jackwuyongxing/p/3514479.html) uses a similar algorithm to split a numberâ€”if youâ€™re interested, you can check it out.



## Shuffling a deck of cards

To be honest, I did this problem back when I first learned C in school. I donâ€™t remember if it was from some self-study video or a textbook I bought. But because I was nervous, I didnâ€™t come up with a good method. I was so nervous when I heard the question that I even asked the interviewer to repeat the scenario.... Actually I was just trying to buy time so I could think more, but I could feel the interviewer rolling his eyes.... This is also a downside of phone interviews: itâ€™s awkward to stay silent for a long time while thinking. With video or onsite interviews, you can at least make eye contact or let the other person see you thinking.

In the end I cautiously asked if I could use the JDK API. The interviewer was friendly and said yes, so I gave `Collections.shuffle();`. He asked if I knew how it was implemented. I really didnâ€™tâ€”I could only say I hadnâ€™t read that part of the source code. The interviewer probably wanted me to implement shuffle() first. After the interview I checked: it uses Random() to do a random shuffle. Sighâ€”if I had been calmer, my impression wouldâ€™ve been better. During the interview I also mentioned doing it with multithreading. I think that could work too, but I didnâ€™t have a solid idea at the time so I didnâ€™t continue. Here Iâ€™ll share the code I thought through afterwards.

```java
  /**
   * èŠ±è‰²çš„list
   */
  List<String> colors = Lists.newArrayList("é»‘æ¡ƒ", "çº¢å¿ƒ", "æ¢…èŠ±", "æ–¹å—");

  /**
   * å­—é¢å€¼çš„list. ä½¿ç”¨åŒæµåˆå¹¶, å› ä¸ºå¾ˆæ‡’ä¸æ„¿æ„æ‰“2-10, å°±ç”¨IntStreamæ¥ç”Ÿæˆäº†
   */
  List<String> numbers = Stream.concat(IntStream.rangeClosed(2, 10).mapToObj(String::valueOf),
      Stream.of("A", "J", "K", "Q")).collect(Collectors.toList());

```

**First define the structure: two Sets, one for suits and one for face values**. Reminder: the code is uploaded to [github](https://github.com/oreoft/blog-examples/tree/main/interview-design1125).

### Collections.shuffle()

```java
  @Test
  public void collectionShuffleTest() {
    // è¿›è¡Œç»„åˆ
    List<String> result = colors.stream()
        .flatMap(color -> numbers.stream().map(number -> color + number))
        .collect(Collectors.toList());

    // è¿›è¡Œæ‰“ä¹±
    Collections.shuffle(result);
    // åœ¨consoleè¾“å‡º(ä¸ºäº†æ–¹ä¾¿çœ‹, è½¬æˆprettyçš„json)
    System.out.println(JSON.toJSONString(result, SerializerFeature.PrettyFormat));
  }
```



### Random swap

```java
  @Test
  public void randomSwapTest() {
    // è¿›è¡Œç»„åˆ
    List<String> result = colors.stream()
        .flatMap(color -> numbers.stream().map(number -> color + number))
        .collect(Collectors.toList());

    // è¿›è¡Œæ‰“ä¹±52æ¬¡
    for (int count = result.size(); count > 0; count--) {
      // ç”Ÿæˆå¯¹æ¢çš„éšæœºæ•°
      int site1 = RandomUtil.randomInt(0, result.size() - 1);
      int site2 = RandomUtil.randomInt(0, result.size() - 1);
      // è¿›è¡Œwap
      String temp = result.get(site1);
      result.set(site1, result.get(site2));
      result.set(site2, temp);
    }

    // åœ¨consoleè¾“å‡º(ä¸ºäº†æ–¹ä¾¿çœ‹, è½¬æˆprettyçš„json)
    System.out.println(JSON.toJSONString(result, SerializerFeature.PrettyFormat));
  }
```

The code is actually very simpleâ€”basically like writing business code. How did I not think of it on the spot? But looking at it, I feel the space complexity is pretty high: on one hand you need to generate two random indices each time; on the other hand each swap needs extra space. But since this isnâ€™t numeric swapping, thereâ€™s no way to optimize with bit operations. And I also looked into how `Collections.shuffle()` is implementedâ€”itâ€™s pretty similar to mine.

```java
/**
 * Swaps the two specified elements in the specified array.
 */
private static void swap(Object[] arr, int i, int j) {
    Object tmp = arr[i];
    arr[i] = arr[j];
    arr[j] = tmp;
}
```

**If I come up with a better approach next time, Iâ€™ll add it here.**



### Trying multithreading

Because itâ€™s random in nature, I thought about a multithreaded unfair-lock contention approach. If you put the items to be shuffled into **one** queue, then no matter how you execute it, itâ€™s still orderedâ€”unless you read randomly or place randomly, but then it has nothing to do with multithreading.

The simplest version is to use the JDKâ€™s `ForkJoinPool.commonPool()` thread pool, and use Stream parallelism to scatter each element and concurrently add them into the final result container. In practice, maybe due to low parallelism, it can shuffle somewhat, but you still see many consecutive suits. You could replace the Stream thread pool and tune the parallelism yourself; the effect should be much better.

```java
@Test
@SuppressWarnings("all")
public void parallelStreamTest() {
  // å…ˆæ„å»ºæ‰‘å…‹ç‰Œ
  List<String> rawList = colors.stream()
      .flatMap(color -> numbers.stream().map(number -> color + number))
      .collect(Collectors.toList());

  // åˆ›å»ºå®¹å™¨
  List<String> result = new ArrayList<>(sumCount);

  // ä½¿ç”¨å‰¯ä½œç”¨è¿›è¡Œå¹¶è¡Œæ·»åŠ åˆ°å®¹å™¨ä¸­
  rawList.parallelStream().forEach(result::add);

  // åœ¨consoleè¾“å‡º(ä¸ºäº†æ–¹ä¾¿çœ‹, è½¬æˆprettyçš„json)
  System.out.println(JSON.toJSONString(result, SerializerFeature.PrettyFormat));
  System.out.println(result.size());
}
```



Then I tried implementing it myself and found it way harder than I expected. I thought about grouping all cards by suit, then having four groups (threads) compete for a `ReentrantLock` unfair lock, and using `condition` to control random order. Each group would be a Queue containing all cards of that suit, and each time a thread gets picked randomly it dequeues one element into the result container. I also thought about splitting into suit groups and face-value groups, then having each suit group scan face values in parallel, and face values also being a queue where you dequeue one each time. But checking boundary conditions and operation logic couldnâ€™t guarantee atomicity, and thread-safety issues kept popping up. After thinking for an entire afternoon, I ended up with this approach:

```java
 @Test
  @SneakyThrows
  @SuppressWarnings("all")
  public void multithreadingTest() {
    // æŠŠèŠ±è‰²å…ˆåˆ†å¥½
    List<Queue<String>> rawList = colors.stream()
        .map(color -> numbers.stream().map(number -> color + number)
            .collect(Collectors.toCollection(LinkedList::new)))
        .collect(Collectors.toList());

    List<String> result = new ArrayList<>(sumCount);
    // å·æ‡’ä½¿ç”¨jdkè‡ªå¸¦çš„çº¿ç¨‹æ± 
    ExecutorService executor = Executors.newCachedThreadPool();
    CountDownLatch countDownLatch = new CountDownLatch(colorCount);

    for (int i = 0; i < colorCount; i++) {
      int current = i;
      ReentrantLock lock = new ReentrantLock();
      for (int count = 1; count <= numberCount; count++) {
        executor.execute(() -> {
          lock.lock();
          if (CollectionUtil.isNotEmpty(rawList.get(current))) {
            result.add(rawList.get(current).poll());
          }
          lock.unlock();
        });
      }
      countDownLatch.countDown();
    }

    // é˜²æ­¢ä¸»çº¿ç¨‹æå‰ç»“æŸ
    countDownLatch.await(10, TimeUnit.SECONDS);

    // åœ¨consoleè¾“å‡º(ä¸ºäº†æ–¹ä¾¿çœ‹, è½¬æˆprettyçš„json)
    System.out.println(JSON.toJSONString(result, SerializerFeature.PrettyFormat));
    System.out.println(result.size());
  }
```

Just to be clear: this has thread-safety issues and canâ€™t reliably behave as expected every time. But after grinding for an afternoon I still needed to give myself something to show for it. When I have time later Iâ€™ll keep optimizing it. I also hope the experts here can give me some ideas or point out the problems.



## Afterword

These are all pretty simple problems, and none of them have a single standard answer. When you write programs, you can always brute-force something naive and still get AC. But the thinking, the approaches, and the complexity analysis are the most interesting parts. When I have time, I want to keep digging into them.

## References

1. [A brief introduction to WeChat red packet architecture design](https://gameinstitute.qq.com/community/detail/104216)
2. [Implementing the double-mean algorithm and red packet grabbing in golang](https://www.php.cn/be/go/458065.html)

3. [Linear optimal splitting algorithm](https://www.cnblogs.com/jackwuyongxing/p/3514479.html)